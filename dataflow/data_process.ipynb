{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理与存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本说明\n",
    "\n",
    "脚本根据 [citation-recommendation](https://github.com/whuscity/citation-recommendation) 整理。\n",
    "\n",
    "本脚本的作用：\n",
    "* 读取原始的引文数据\n",
    "* 引文、作者、关键词 ID 化\n",
    "* 根据网络表示学习不同模型特点，保存为模型所需的连边、网络等\n",
    "* 将基础数据存入数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依赖\n",
    "\n",
    "图表示部分基于 [@shenweichen](https://github.com/shenweichen) 的 [GraphEmbedding](https://github.com/shenweichen/GraphEmbedding) 中的部分模型，可参考该仓库进行导入。\n",
    "\n",
    "执行本脚本所需包如下，一并导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "import sys,argparse,random\n",
    "from gensim.models import Word2Vec\n",
    "from ge import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件目录、数据库配置限定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_input = \"./data/aan/aan_full.csv\"\n",
    "dir_node_type = \"./data/aan/aan_node_type.txt\"\n",
    "dir_gatne_edge = \"./data/aan/aan_gatne_edge.txt\"\n",
    "dir_normal_edge = \"./data/aan/aan_normal_edge.txt\"\n",
    "dir_emb = \"./data/aan/aan_node2vec.emb\"\n",
    "dir_cite_edge = \"./data/aan/aan_cite_edge.txt\"\n",
    "dir_cite_nx = \"./data/aan/aan_cite.edgelist\"\n",
    "dir_final_data = \"./data/aan/aan.csv\"\n",
    "dir_node_id = \"./data/aan/aan_node_id.csv\"\n",
    "\n",
    "dbname = \"aan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship, NodeMatcher, RelationshipMatcher\n",
    "\n",
    "class neoConnection:\n",
    "    def __init__(self):\n",
    "        self.graph = Graph(\n",
    "            \"http://localhost:7474\",\n",
    "            user = \"cradmin\",\n",
    "            password = \"888888\"\n",
    "        )\n",
    "\n",
    "import pymysql\n",
    "\n",
    "class mysqlConnection:\n",
    "    def __init__(self):\n",
    "        self.connection = pymysql.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"cradmin\",\n",
    "            passwd=\"888888\",\n",
    "            database=\"cit_rec\"\n",
    "        )\n",
    "        self.cursor = self.connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "## 数据读取与描述统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir_input,usecols = ['id','author','refs','new_new_keywords','year','title','abstract','content'])\n",
    "authors = list(set(df['author']))\n",
    "keywords = list(set(df['new_new_keywords']))\n",
    "refs = list(set(df['refs']))\n",
    "papers = list(set(df['id'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不重复的论文数为16706\n",
      "不重复的作者数为18861\n",
      "不重复的关键词数为44502\n"
     ]
    }
   ],
   "source": [
    "aus = []\n",
    "for au in authors:\n",
    "    if type(au)!=float and len(au)>0:\n",
    "        aus += au.split(\";\")\n",
    "aus = list(set(aus))\n",
    "kws = []\n",
    "for kw in keywords:\n",
    "    if type(kw)!=float and len(kw)>0:\n",
    "        kws += kw.split(\";\")\n",
    "kws = list(set(kws))\n",
    "paper_count = len(papers)\n",
    "author_count = len(set(aus))\n",
    "keyword_count = len(set(kws))\n",
    "print(\"不重复的论文数为{}\".format(paper_count))\n",
    "print(\"不重复的作者数为{}\".format(author_count))\n",
    "print(\"不重复的关键词数为{}\".format(keyword_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体 ID 化\n",
    "\n",
    "统计完论文、作者、关键词的数量之后就可以对其进行 ID 化。\n",
    "创建一个 `node_type` 文件用于记录不同 ID 的节点实体类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []; a = []; k = []\n",
    "for i in range(paper_count):\n",
    "    p.append(str(i)+\" \"+\"P\")\n",
    "for j in range(paper_count,paper_count+author_count):\n",
    "    a.append(str(j)+\" \"+\"A\")\n",
    "for m in range(paper_count+author_count,paper_count+author_count+keyword_count):\n",
    "    k.append(str(m)+\" \"+\"K\")\n",
    "total = p+a+k\n",
    "with open(dir_node_type,\"w\") as f:\n",
    "    for each in total:\n",
    "        f.write(each+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(x,entity_name_set,offset):\n",
    "    entity_ids = []\n",
    "    if type(x) != float and len(x)>0:\n",
    "        for entity_name in x.split(';'):\n",
    "            entity_ids.append(str(entity_name_set.index(str(entity_name))+offset))\n",
    "    return ';'.join(entity_ids)\n",
    "\n",
    "df['paper_id'] = df['id'].apply(get_id,entity_name_set=papers,offset=0)\n",
    "df['author_id'] = df['author'].apply(get_id,entity_name_set=aus,offset=paper_count)\n",
    "df['keyword_id'] = df['new_new_keywords'].apply(get_id,entity_name_set=kws,offset=paper_count+author_count)\n",
    "df['refs_id'] = df['refs'].apply(get_id,entity_name_set=papers,offset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成连边数据\n",
    "\n",
    "目前包含三类连边：\n",
    "1. 【作者】----(撰写)----【论文】\n",
    "2. 【论文】----(包含)----【关键词】\n",
    "3. 【论文】----(引用)----【论文】\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_p_edges = []; normal_a_p_edges = []\n",
    "p_k_edges = []; normal_p_k_edges = []\n",
    "p_p_edges = []; normal_p_p_edges = []\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    if len(row['author_id']) > 0:\n",
    "        for author in row['author_id'].split(';'):\n",
    "            a_p_edges.append(\"1 \" + str(author) + ' ' + str(row['paper_id']))\n",
    "            normal_a_p_edges.append(str(author) + ' ' + str(row['paper_id']))\n",
    "    if len(row['keyword_id']) > 0:\n",
    "        for keyword in row['keyword_id'].split(';'):\n",
    "            p_k_edges.append(\"2 \" + str(row['paper_id']) + ' ' + str(keyword))\n",
    "            normal_p_k_edges.append(str(row['paper_id']) + ' ' + str(keyword))\n",
    "    if len(row['refs_id']) > 0:\n",
    "        for ref in row['refs_id'].split(';'):\n",
    "            p_p_edges.append(\"3 \" + str(row['paper_id']) + ' ' + str(ref))\n",
    "            normal_p_p_edges.append(str(row['paper_id']) + ' ' + str(ref))\n",
    "\n",
    "def write_text(texts,filepath):\n",
    "    with open(filepath,\"w\",encoding = \"utf-8\") as f:\n",
    "        for each in texts:\n",
    "            f.write(each+\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "gatne_edges = a_p_edges + p_k_edges + p_p_edges\n",
    "normal_edges = normal_a_p_edges + normal_p_k_edges + normal_p_p_edges\n",
    "write_text(gatne_edges,dir_gatne_edge)\n",
    "write_text(normal_edges,dir_normal_edge)\n",
    "write_text(normal_p_p_edges,dir_cite_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络表示学习\n",
    "\n",
    "根据 [@flyingzhy](https://github.com/flyingzhy) 的试验结果直接采取最优状态下的模型参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取 `networkx` 图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "        res = f.readlines()\n",
    "        f.close()\n",
    "    pairs = []\n",
    "    for edge in res:\n",
    "        edge_list = edge.strip(\"\\n\").split(\" \")\n",
    "        pairs.append((int(edge_list[0]),int(edge_list[1])))\n",
    "    return pairs\n",
    "path = nx.DiGraph()\n",
    "edges = read_txt(dir_cite_edge)\n",
    "path.add_edges_from(edges)\n",
    "dedges = path.edges()\n",
    "ans = []\n",
    "for each in edges:\n",
    "    if each not in dedges:\n",
    "        ans.append(each)\n",
    "nx.write_edgelist(path, dir_cite_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec 表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 5\n",
      "2 / 5\n",
      "3 / 5\n",
      "4 / 5\n",
      "5 / 5\n",
      "开始执行\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python38\\Lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存完毕\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    '''\n",
    "    Parses the node2vec arguments.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(description=\"Run node2vec.\")\n",
    "\n",
    "    parser.add_argument('--input', default=dir_cite_nx,\n",
    "                        help='Input graph path')\n",
    "\n",
    "    parser.add_argument('--output', default=dir_emb,\n",
    "                        help='Embeddings path')\n",
    "\n",
    "    parser.add_argument('--dimensions', type=int, default=128,\n",
    "                        help='Number of dimensions. Default is 128.')\n",
    "\n",
    "    parser.add_argument('--walk-length', type=int, default=20,\n",
    "                        help='Length of walk per source. Default is 80.')\n",
    "\n",
    "    parser.add_argument('--num-walks', type=int, default=5,\n",
    "                        help='Number of walks per source. Default is 10.')\n",
    "\n",
    "    parser.add_argument('--window-size', type=int, default=8,\n",
    "                        help='Context size for optimization. Default is 10.')\n",
    "\n",
    "    parser.add_argument('--iter', default=1, type=int,\n",
    "                        help='Number of epochs in SGD')\n",
    "\n",
    "    parser.add_argument('--workers', type=int, default=8,\n",
    "                        help='Number of parallel workers. Default is 8.')\n",
    "\n",
    "    parser.add_argument('--p', type=float, default=0.5,\n",
    "                        help='Return hyperparameter. Default is 1.')\n",
    "\n",
    "    parser.add_argument('--q', type=float, default=0.5,\n",
    "                        help='Inout hyperparameter. Default is 1.')\n",
    "\n",
    "    parser.add_argument('--weighted', dest='weighted', action='store_true',\n",
    "                        help='Boolean specifying (un)weighted. Default is unweighted.')\n",
    "    parser.add_argument('--unweighted', dest='unweighted', action='store_false')\n",
    "    parser.set_defaults(weighted=False)\n",
    "\n",
    "    parser.add_argument('--directed', dest='directed', action='store_true',\n",
    "                        help='Graph is (un)directed. Default is undirected.')\n",
    "    parser.add_argument('--undirected', dest='undirected', action='store_false')\n",
    "    parser.set_defaults(directed=False)\n",
    "\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "def read_graph():\n",
    "    '''\n",
    "    Reads the input network in networkx.\n",
    "    '''\n",
    "    if args.weighted:\n",
    "        G = nx.read_edgelist(args.input, nodetype=int, data=(('weight', float),), create_using=nx.DiGraph())\n",
    "    else:\n",
    "        G = nx.read_edgelist(args.input, nodetype=int, create_using=nx.DiGraph())\n",
    "        for edge in G.edges():\n",
    "            G[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "    if not args.directed:\n",
    "        G = G.to_undirected()\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def learn_embeddings(walks):\n",
    "    '''\n",
    "    Learn embeddings by optimizing the Skipgram objective using SGD.\n",
    "    '''\n",
    "    walks = [list(map(str, walk)) for walk in walks]\n",
    "    model = Word2Vec(walks, size=args.dimensions, window=args.window_size, min_count=0, sg=1, workers=args.workers,\n",
    "                     iter=args.iter)\n",
    "    model.wv.save_word2vec_format(args.output)\n",
    "    print(\"保存完毕\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "nx_G = read_graph()\n",
    "G = Graph(nx_G, args.directed, args.p, args.q)\n",
    "G.preprocess_transition_probs()\n",
    "walks = G.simulate_walks(args.num_walks, args.walk_length)\n",
    "print(\"开始执行\")\n",
    "learn_embeddings(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(dir_final_data,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-75511d67b4db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnode_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mnode_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnode_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "node_list = []\n",
    "for x,y in zip(np.array(range(0,paper_count)),np.array(papers)):\n",
    "    node_list.append([x,y,'P'])\n",
    "for x,y in zip(np.array(range(paper_count,paper_count+author_count)),np.array(aus)):\n",
    "    node_list.append([x,y,'A'])\n",
    "for x,y in zip(np.array(range(paper_count+author_count,paper_count+author_count+keyword_count)),np.array(kws)):\n",
    "    node_list.append([x,y,'K'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame(node_list,columns=['id','node_name','node_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['dbname']=dbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_csv(dir_node_id,index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
